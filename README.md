# ğŸ§  Py-DataEngineering Workshop

**From CSV to Dashboard â€” Building a Mini Data Pipeline in Pure Python**

This workshop shows how to build a complete **data engineering workflow** using open-source tools:  
`pandas â€¢ pyarrow â€¢ uv â€¢ streamlit`.

Youâ€™ll ingest, clean, model, and visualize the classic **Northwind** dataset â€” all locally, with no cloud or database setup.

---

## ğŸ§­ What This Repo *Is* â€” and *Isnâ€™t*

### âœ… What It *Is*
- A **hands-on sandbox** for learning how data engineers think and structure pipelines.  
- A **mini data warehouse** built entirely with open-source tools.  
- A practical example of the modern data-engineering mindset:  
  **Extract â†’ Load â†’ Transform â†’ Build â†’ Visualize.**  
- Designed for:
  - Beginners exploring data pipelines and analytics engineering.  
  - Educators or mentors leading workshops and bootcamps.  
  - Teams wanting a lightweight demo of ETL + Data Quality in Python.

### ğŸš« What It *Isnâ€™t*
- âŒ Not a production-grade data platform or big-data tool.  
- âŒ Not a replacement for Airflow, dbt, or Spark.  
- âŒ Not built for parallel or distributed execution.  
- âŒ Not a full data-engineering curriculum â€” this is the *first step*.

## ğŸ’¡ In One Line

> A **teachable microcosm** of modern data engineering â€” small enough for your laptop, structured enough to mirror real-world pipelines.

---

## ğŸ“¦ Project Structure

```

py-dataengineering-workshop/
â”œâ”€ data/
â”‚  â”œâ”€ 00-raw/       # raw source CSVs (auto-downloaded)
â”‚  â”œâ”€ 01-clean/     # cleaned + validated Parquet
â”‚  â”‚  â””â”€ _dq/       # data quality logs
â”‚  â”œâ”€ 02-model/     # modeled & aggregated tables
â”‚  â””â”€ 03-sandbox/   # scratch area
â”œâ”€ etl/
â”‚  â”œâ”€ extract.py    # download + read CSVs
â”‚  â”œâ”€ load.py       # clean + validate
â”‚  â”œâ”€ transform.py  # create fact/dim tables
â”‚  â”œâ”€ build.py      # aggregate to gold layer
â”‚  â”œâ”€ dq.py         # data-quality rules + logs
â”‚  â”œâ”€ paths.py      # central folder definitions
â”‚  â””â”€ run.py        # orchestrator (CLI)
â””â”€ view_data.py     # Inspect parquet data files
â””â”€ app.py           # Streamlit dashboard

````

---

## âš™ï¸ Quickstart

```bash
git clone https://github.com/YOURNAME/py-dataengineering-workshop.git
cd py-dataengineering-workshop
uv venv && uv sync

# Run the full pipeline
uv run python -m etl.run

# Or run by stage
uv run python -m etl.run --stage extract
uv run python -m etl.run --stage load
uv run python -m etl.run --stage transform
uv run python -m etl.run --stage build

# Launch the dashboard
uv run streamlit run app.py
````

On first run, `extract.py` downloads the Northwind CSVs into `data/00-raw/`.

---

## ğŸ§© Pipeline Overview

| Stage         | Script             | Purpose                                                   |
| ------------- | ------------------ | --------------------------------------------------------- |
| **Extract**   | `etl/extract.py`   | Download + load raw CSVs                                  |
| **Load**      | `etl/load.py`      | Clean, standardize, and log data-quality issues           |
| **Transform** | `etl/transform.py` | Create fact and dimension tables                          |
| **Build**     | `etl/build.py`     | Aggregate gold-layer outputs (customer, country, product) |

Data-quality results are saved under:

```
data/01-clean/_dq/
â”œâ”€ dq_runs.parquet
â””â”€ dq_issues.parquet
```
---

### ğŸ§° Quick Parquet Viewer

You can quickly inspect any Parquet file generated by the ETL pipeline using `pandas + pyarrow`.

```bash
uv run python view_data.py
```

This utility prints:

* ğŸ“‚ The file being read
* ğŸ‘€ A preview of the first 5 rows (`head()`)
* ğŸ§¾ Schema and data types (`info()`)
* ğŸ“Š Summary statistics (`describe()`)

To view another file, edit the `target` path inside `view_data.py`
(e.g., switch from `sales_by_customer.parquet` to `fact_sales.parquet`).


---

## ğŸ“Š Streamlit Dashboard

```bash
uv run streamlit run app.py
```

Open [http://localhost:8501](http://localhost:8501)

**Tabs**

1. ğŸ“Š **Sales (Customers)** â€“ Top customers & products
2. ğŸŒ **Sales by Country** â€“ Regional aggregates
3. ğŸ§ª **Data Quality** â€“ Run summaries & issue details

---
## ğŸŒ» Challenge Yourself!

1. Study the [Northwind dataset](https://en.wikiversity.org/wiki/Database_Examples/Northwind).
2. Modify `extract.py` to ingest all available Northwind CSVs (Employees, Shippers, Suppliers, Categories, etc.).
3. Add new data-quality checks in `dq.py` (e.g., missing employee names, invalid postal codes).
4. Extend `transform.py` to include `dim_product`, `dim_supplier`, and `fact_orders`.
5. Enhance `build.py` with new aggregates (e.g., sales by category, supplier, year).
6. Visualize additional metrics in Streamlit (e.g., monthly trends, top-selling categories).
7. Use another dataset (like, [Chinook](https://www.sqlitetutorial.net/sqlite-sample-database/)) and create and test it through the pipeline.
8. Grow beyond Streamlit and try another web framework like [Air](http://feldroy.github.io/air/).

---

### ğŸª¶ Air Dashboard (Streamlit Expansion)

This app reimagines the original **Streamlit Northwind Dashboard** using **Air**, a FastAPI-based web framework.
Air expands on Streamlit by enabling fully web-native dashboardsâ€”mixing HTML, APIs, and charts in one lightweight Python app.

Run locally with:

```bash
uv run uvicorn air_app:app --reload
```

---

## ğŸŒ± From Demo â†’ Real-World Data Stack

| Stage                 | What it does                     | Recommended tools & patterns                                                                  |
| --------------------- | -------------------------------- | --------------------------------------------------------------------------------------------- |
| **Extract**           | Pull data from CSV/API/DB/stream | **dlt** (Python) â†’ sources (CSV, REST, DB). For streams: **Kafka** (later).                   |
| **Load**              | Land raw/staging into warehouse  | **dlt â†’ Postgres** (Dockerized). For dev-only: optionally DuckDB parquet.                     |
| **Transform**         | Create clean staging + marts     | **dbt** (SQL or Python models) over Postgres; incremental models for scale.                   |
| **Store**             | Persist analytical outputs       | **Postgres** (marts schemas), optional **Parquet** in `/02-model` for ad hoc.                 |
| **Serve**             | BI / apps / ad-hoc queries       | **Streamlit** app; **Metabase** (Docker) for dashboards; programmatic **Ibis/DuckDB**.        |
| **Orchestrate**       | Schedule & chain runs            | **Prefect** (flows locally or Cloud). For simple prod cron: **crontab** + shell.              |
| **Validate / DQ**     | Schemas, ranges, FKs             | **Pandera** (Python), selective **Great Expectations**; **dbt tests** (`not_null`, `unique`). |
| **Observe**           | Runs, metrics, alerts            | dlt run metrics + Prefect run states; ship DQ summaries to **Grafana/Metabase/Slack**.        |
| **Version & Lineage** | Repro, history, docs             | **git** for code, optional **dvc** for large artifacts; **dbt docs + lineage**.               |
| **Semantic Layer**    | Uniform query interface          | **Ibis** to query Postgres/Parquet with one API; (optional) **dbt metrics**.                  |
| **ML / Features**     | Gold â†’ features                  | Notebook lab or **feature store** later; start with **pandas/Polars** over marts.             |

Explore these tools to build a deeper appreciation for how Data Engineering scalesâ€”experiment with various data sources and implement your own end-to-end projects.

---


## ğŸ§¾ License

MIT Â© 2025 Myk Ogbinar / Data Engineering Pilipinas

---

## ğŸ™Œ Acknowledgments

* [Neo4j Northwind Dataset](https://github.com/neo4j-contrib/northwind-neo4j) â€” Sample dataset used for this project
* [pandas](https://pandas.pydata.org/) â€¢ [pyarrow](https://arrow.apache.org/docs/python/) â€” Core data processing and Parquet handling
* [streamlit](https://streamlit.io/) â€” Interactive data app framework
* [uv](https://github.com/astral-sh/uv) â€” Fast Python environment and dependency manager
* [dltHub](https://dlthub.com/) â€¢ [dbt](https://www.getdbt.com/) â€¢ [DuckDB](https://duckdb.org/) â€” Modern ELT and analytical data stack components
* [Ibis](https://ibis-project.org/) â€¢ [ClickHouse](https://clickhouse.com/) â€” Unified analytical querying and columnar storage engines
* [DurianPy](https://durianpy.org/) â€¢ [PyCon Davao 2025](https://pycon-davao.durianpy.org/) â€” Community and conference initiatives supporting open-source learning
* [Data Engineering Pilipinas](https://dataengineering.ph/) â€” Open-source community promoting data literacy and collaboration in the Philippines

