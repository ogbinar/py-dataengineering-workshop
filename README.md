# ğŸ§  Py-DataEngineering Workshop

**From CSV to Dashboard â€” Building a Mini Data Pipeline in Pure Python**

This workshop demonstrates how to build a full **data engineering workflow** using only open-source tools:  
`pandas â€¢ pyarrow â€¢ uv â€¢ streamlit`.

Youâ€™ll ingest, clean, model, and visualize the classic **Northwind** dataset â€” all locally, no cloud or database required.

---

## ğŸ“¦ Project Structure

```

py-dataengineering-workshop/
â”œâ”€ README.md
â”œâ”€ pyproject.toml
â”œâ”€ .gitignore
â”œâ”€ data/
â”‚  â”œâ”€ 00-raw/         # raw source CSVs (downloaded automatically)
â”‚  â”œâ”€ 01-clean/       # cleaned + validated Parquet data
â”‚  â”‚  â””â”€ _dq/         # data quality logs
â”‚  â”œâ”€ 02-model/       # modeled & aggregated Parquet tables
â”‚  â””â”€ 03-sandbox/     # optional scratch area
â”œâ”€ etl/
â”‚  â”œâ”€ extract.py      # downloads + reads raw CSVs
â”‚  â”œâ”€ load.py         # cleans data + runs DQ checks
â”‚  â”œâ”€ transform.py    # creates modeled tables (fact/dim)
â”‚  â”œâ”€ build.py        # aggregates & writes gold layer
â”‚  â”œâ”€ dq.py           # reusable DQ rules + logging
â”‚  â”œâ”€ paths.py        # centralized folder definitions
â”‚  â””â”€ run.py          # stage orchestrator (CLI)
â””â”€ app.py             # Streamlit dashboard

````

---

## ğŸ§° Requirements

- Python â‰¥ 3.10  
- [uv](https://github.com/astral-sh/uv) (modern fast package/environment manager)  
- Internet connection (for first-time Northwind CSV download)

---

## âš™ï¸ Setup

```bash
# 1. Clone this repo
git clone https://github.com/YOURNAME/py-dataengineering-workshop.git
cd py-dataengineering-workshop

# 2. Create environment + install deps
uv venv && uv sync

# 3. (Optional) fetch data manually
uv run python -m etl.extract

# 4. Run the entire pipeline
uv run python -m etl.run
````

On first run, the `extract` stage will automatically download:

```
Customers.csv, Orders.csv, Order_Details.csv, Products.csv
```

and store them in `data/00-raw/`.

---

## ğŸ§© Pipeline Overview

| Stage         | Script             | Purpose                                                       |
| ------------- | ------------------ | ------------------------------------------------------------- |
| **Extract**   | `etl/extract.py`   | Download + load raw CSVs into memory                          |
| **Load**      | `etl/load.py`      | Clean data, standardize columns, and log data-quality issues  |
| **Transform** | `etl/transform.py` | Create fact/dimension tables                                  |
| **Build**     | `etl/build.py`     | Aggregate to gold layer (sales by customer, country, product) |

Run any stage individually:

```bash
uv run python -m etl.run --stage extract
uv run python -m etl.run --stage load
uv run python -m etl.run --stage transform
uv run python -m etl.run --stage build
```

---

## ğŸ§ª Data Quality

All validation results are stored in:

```
data/01-clean/_dq/
â”œâ”€ dq_runs.parquet    # summary of each run
â””â”€ dq_issues.parquet  # detailed issue list
```

Rules include:

* Non-null & positive IDs
* Valid price, quantity, discount ranges
* Foreign-key consistency (Orders â†” Customers)

---

## ğŸ“Š Streamlit Dashboard

After running the pipeline:

```bash
uv run streamlit run app.py
```

Then open [http://localhost:8501](http://localhost:8501).

**Tabs:**

1. ğŸ“Š **Sales (Customers)** â€“ Top customers & products
2. ğŸŒ **Sales by Country** â€“ Aggregated view by region
3. ğŸ§ª **Data Quality** â€“ Latest DQ run and issue details

---

## ğŸ§± Teaching Flow

This repo is structured for live code-along sessions:

1. **Inspect raw CSVs** â†’ `extract.py`
2. **Clean & validate** â†’ `load.py`
3. **Model & aggregate** â†’ `transform.py` + `build.py`
4. **Visualize** â†’ `app.py` (Streamlit)

Each layer is self-contained, readable, and directly runnable.

---

## ğŸ’¡ Extensions

* Replace Northwind with your own dataset
* Add new DQ rules in `dq.py`
* Create extra dashboards (e.g., by category or year)
* Swap pandas â†’ Polars or DuckDB to compare performance

---

## ğŸ§¾ License

MIT Â© 2025 Myk Ogbinar / Data Engineering Pilipinas

---

## ğŸ™Œ Acknowledgments

* [Neo4j Northwind Dataset](https://github.com/neo4j-contrib/northwind-neo4j)
* [pandas](https://pandas.pydata.org/)
* [Streamlit](https://streamlit.io/)
* [DurianPy](https://durianpy.org/)
* [PyCon Davao 2025](https://pycon-davao.durianpy.org/)
* [Data Engineering Pilipinas](https://dataengineering.ph/)


